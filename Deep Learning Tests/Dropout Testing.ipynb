{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN for Generalized Logistic Regression (Multinomial Logistic Regression)\n",
    "\n",
    "In the following, we will create a Multi-Class Classification Deep Neural Net.  This will also function for binary classification as well since we are using a generalized version of Logistic Regression's activation function, $\\sigma$, which is a *softmax activation* function.\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "### Hyper-Parameters\n",
    "\n",
    "For simplicity, I have used established conventions in deciding the particular hyper-parameters, however, their corresponding values can be adjusted.  The default values are based on concensus amongst the community.\n",
    "\n",
    "Hyper-Parameters and their Values (if applicable)\n",
    "- He Initialization\n",
    "- L2 Regularization $\\lambda : 0.1$\n",
    "- ADAM Optimization \n",
    "$$\\beta_1 : 0.9$$\n",
    "$$\\beta_2 : 0.999$$ \n",
    "$$\\epsilon : 1e-8$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from tf_utils import load_dataset,convert_to_one_hot\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (12288, 1080)\n",
      "Y_train shape: (6, 1080)\n",
      "X_test shape: (12288, 120)\n",
      "Y_test shape: (6, 120)\n",
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x96d1cc0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+A5HBeEL3+GoIHzhOxwVhC98h6OC+P/J5gK+E4O8XgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9651be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Flatten the training and test images\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "# Normalize image vectors\n",
    "X_train = X_train_flatten/255.\n",
    "X_test = X_test_flatten/255.\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6)\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6)\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n",
    "print (Y_train)\n",
    "plt.imshow(X_train_orig[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''## START CODE HERE ##\n",
    "my_image = \"my_image.jpg\" # change this to the name of your image file \n",
    "my_label_y = [1] # the true class of your image (1 -> cat, 0 -> non-cat)\n",
    "## END CODE HERE ##\n",
    "\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(ndimage.imread(fname, flatten=False))\n",
    "my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))\n",
    "my_predicted_image = predict(my_image, my_label_y, parameters)\n",
    "\n",
    "plt.imshow(image)'''\n",
    "\n",
    "def classify(image_loc, actual, parameters):\n",
    "    image = np.array(ndimage.imread(fname, flatten=False))\n",
    "    nimage = scipy.misc.imresize(image, size=(64,64)).reshape((64*64*3,1))\n",
    "    probs = predict(image, parameters)[0]\n",
    "    a = accuracy(probs, actual)\n",
    "    plt.imshow(image)\n",
    "    print (a[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params_he(layers_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1\n",
    "     \n",
    "    for l in range(1, L + 1):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])*np.sqrt(2/layers_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def init_adam(parameters) :\n",
    "    L = len(parameters) // 2\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "    \n",
    "    return v, s\n",
    "def update_par(parameters, grads, learning_rate):\n",
    "    L = len(parameters)//2\n",
    "    for l in range(L):\n",
    "        parameters['W{0}'.format(l+1)] = parameters['W{0}'.format(l+1)] - learning_rate*grads['dW{0}'.format(l+1)]\n",
    "        parameters['b{0}'.format(l+1)] = parameters['b{0}'.format(l+1)] - learning_rate*grads['db{0}'.format(l+1)]\n",
    "    return parameters\n",
    "\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \n",
    "    L = len(parameters) // 2                 \n",
    "    v_corrected = {}                         \n",
    "    s_corrected = {}                         \n",
    "    learning_rate = learning_rate * np.sqrt(1-np.power(beta2, t))/(1-np.power(beta1, t))\n",
    "\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)] + (1-beta1)*grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)] + (1-beta1)*grads[\"db\" + str(l+1)]\n",
    "\n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)] + (1-beta2)*np.square(grads[\"dW\" + str(l+1)])\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)] + (1-beta2)*np.square(grads[\"db\" + str(l+1)])\n",
    "\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*np.divide(v[\"dW\"+str(l+1)],\n",
    "                                                                                          np.sqrt(s[\"dW\"+str(l+1)])+epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*np.divide(v[\"db\"+str(l+1)],\n",
    "                                                                                          np.sqrt(s[\"db\"+str(l+1)])+epsilon)\n",
    "\n",
    "    return parameters, v, s\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    " \n",
    "    np.random.seed(seed)            \n",
    "    m = X.shape[1]                 \n",
    "    mini_batches = []\n",
    "        \n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size)\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:,k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:,k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:,num_complete_minibatches*mini_batch_size:m+1]\n",
    "        mini_batch_Y = shuffled_Y[:,num_complete_minibatches*mini_batch_size:m+1]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "def dropout(A, keep_probs):\n",
    "    D = np.random.rand(A.shape[0], A.shape[1])\n",
    "    D = D<keep_probs\n",
    "    A *= D\n",
    "    A /= keep_probs\n",
    "    return A, D\n",
    "\n",
    "def reverse_dropout(dW, D, keep_probs):\n",
    "    return dW*D[:,:dW.shape[1]]/keep_probs\n",
    "\n",
    "def avg_grad(n_grads, c_grads, l):\n",
    "    n = {}\n",
    "    if c_grads:\n",
    "        for i in c_grads:\n",
    "            n[i] = (c_grads[:,:n_grads.shape[1]]*l + n_grads)/(l+1)\n",
    "        return n\n",
    "    else:\n",
    "        return n_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x)), x\n",
    "def sigmoid_backward(dA, cache):\n",
    "    s = 1/(1+np.exp(-cache))\n",
    "    return dA*s*(1-s)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x), x\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z<=0] = 0\n",
    "    return dZ\n",
    "def softmax(x):\n",
    "    exps = np.exp(x)\n",
    "    return exps/np.sum(exps, axis = 0, keepdims = True), x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_forward(A, W, b):\n",
    "    return W.dot(A)+b, (A,W,b)\n",
    "def lact_forward(A_prev, W, b, activation, keep_probs):\n",
    "    #A_prev, D = dropout(A_prev, keep_probs)\n",
    "    Z, l_cache = l_forward(A_prev, W, b)\n",
    "    if activation == 'relu':\n",
    "        A, a_cache = relu(Z)\n",
    "    elif activation == 'softmax':\n",
    "        A, a_cache = softmax(Z)\n",
    "    elif activation == 'sigmoid':\n",
    "        A, a_cache = sigmoid(Z)\n",
    "    A, D = dropout(A, keep_probs)\n",
    "    return A, (l_cache, a_cache, D)\n",
    "def L_forward(X, parameters, classification, keep_probs):\n",
    "    caches = []\n",
    "    D_cache = []\n",
    "    A = X\n",
    "    L = len(parameters)//2\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = lact_forward(A_prev, parameters['W{0}'.format(l)], parameters['b{0}'.format(l)], 'relu', keep_probs)\n",
    "        caches.append(cache)\n",
    "    AL, cache = lact_forward(A, parameters[\"W{0}\".format(L)], parameters[\"b{0}\".format(L)], classification, keep_probs = 1.0)\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_backward(dZ, cache, lambd, m):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    if lambd:\n",
    "        dW = 1./m * np.dot(dZ, A_prev.T) + W*lambd/m\n",
    "    else:\n",
    "        dW = 1./m * np.dot(dZ, A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    return dA_prev, dW, db\n",
    "def lact_backward(dA, cache, lambd, m, activation, keep_probs):\n",
    "    l_cache, a_cache, D = cache\n",
    "    if activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA, a_cache)\n",
    "    elif activation == 'relu':\n",
    "        dZ = relu_backward(dA, a_cache)\n",
    "    dA_prev, dW, db = l_backward(dZ, l_cache, lambd, m)\n",
    "    dW = reverse_dropout(dW, D, keep_probs)\n",
    "    return dA_prev, dW, db\n",
    "def L_backward(AL, Y, caches, lambd, m, classification, keep_probs):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    current_cache = caches[L-1]\n",
    "    if classification == 'softmax':\n",
    "        dZ = (AL-Y)#/AL.shape[0]\n",
    "        grads[\"dA{0}\".format(L)], grads['dW{0}'.format(L)], grads['db{0}'.format(L)] = l_backward(dZ, current_cache[0], \n",
    "                                                                                                  lambd, m)\n",
    "    elif classification == 'sigmoid':\n",
    "        dAL = - (np.divide(Y,AL) - np.divide(1-Y, 1-AL))\n",
    "        grads[\"dA{0}\".format(L)], grads['dW{0}'.format(L)], grads['db{0}'.format(L)] = lact_backward(dAL, current_cache, \n",
    "                                                                                                     'sigmoid', lambd, m)\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        grads['dA{0}'.format(l+1)], grads['dW{0}'.format(l+1)], grads['db{0}'.format(l+1)] = lact_backward(grads['dA{0}'.format(l+2)], \n",
    "                                                                                                           current_cache,\n",
    "                                                                                                           lambd, m, \n",
    "                                                                                                           activation = 'relu',\n",
    "                                                                                                           keep_probs = keep_probs)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(inp, parameters):\n",
    "    probs, weights= L_forward(inp, parameters, classification = 'softmax')\n",
    "    return (probs==probs.max(axis = 0, keepdims=1)).astype(float), weights\n",
    "def accuracy(preds, actual):\n",
    "    return sum(sum(preds == actual)==preds.shape[0])/preds.shape[1], preds\n",
    "def compute_cost(preds, actual):\n",
    "    ind = np.argmax(actual, axis = 0)\n",
    "    log_probs = np.log(np.choose(ind, preds))\n",
    "    return -1*np.sum(log_probs)/len(log_probs)\n",
    "def l2_loss(params, lambd):\n",
    "    weight_loss = []\n",
    "    for i in range(len(params)//2):\n",
    "        weight_loss.append(0.5*lambd*np.sum(np.square(params['W{0}'.format(i+1)])))\n",
    "    return sum(weight_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model(X, Y, X_test, Y_test, layer_dims, mini_batch_size = 32, learning_rate = 0.0075, \n",
    "            beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, epoch = 3000, keep_probs = 0.5,\n",
    "            lambd = 0.1, print_cost=True, plot=True, classification = 'sigmoid'):\n",
    "    print\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    ac = []\n",
    "    parameters = init_params_he(layer_dims)\n",
    "    v, s = init_adam(parameters)\n",
    "    t = 0\n",
    "    seed = 10\n",
    "    for i in range(epoch):\n",
    "        seed += 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        c_grads = None\n",
    "        for l, minibatch in enumerate(minibatches):\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            AL, caches = L_forward(minibatch_X, parameters, classification, keep_probs)\n",
    "            c = compute_cost(AL, minibatch_Y)\n",
    "            if lambd:\n",
    "                c += l2_loss(parameters, lambd)\n",
    "            c_grads = avg_grad(L_backward(AL, minibatch_Y, caches, lambd, minibatch_X.shape[1], classification, keep_probs), c_grads, l)\n",
    "            t+=1\n",
    "            parameters, v, s = update_parameters_with_adam(parameters, c_grads, v, s, t, learning_rate, beta1, beta2,  epsilon)\n",
    "        if print_cost and i%20==0:\n",
    "            print (\"Cost after epoch {0}: {1}\".format(i, c))\n",
    "            costs.append(c)\n",
    "            ac_train = accuracy(predict(X, parameters)[0], Y)[0]\n",
    "            ac_test = accuracy(predict(X_test, parameters)[0], Y_test)[0]\n",
    "            print (\"Training Set Accuracy after epoch {0}: {1}\".format(i, ac_train))\n",
    "            print (\"Test Set Accuracy after epoch {0}: {1}\".format(i, ac_test))\n",
    "            ac.append(ac_train)\n",
    "    # plot the cost\n",
    "    if plot:\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        plt.plot(ac)\n",
    "        plt.ylabel('Accuracy Score')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (25,12288) (25,32) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-be06e99bd407>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m p = L_model(X_train, Y_train, X_test, Y_test, mini_batch_size = 32, learning_rate = 0.0001, epoch = 3000, \n\u001b[0;32m      2\u001b[0m             \u001b[0mlayer_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m12288\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m             classification = 'softmax', lambd = None)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-64-98c1e551020d>\u001b[0m in \u001b[0;36mL_model\u001b[1;34m(X, Y, X_test, Y_test, layer_dims, mini_batch_size, learning_rate, beta1, beta2, epsilon, epoch, keep_probs, lambd, print_cost, plot, classification)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml2_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mc_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mt\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_parameters_with_adam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-59-0f932a22e56d>\u001b[0m in \u001b[0;36mL_backward\u001b[1;34m(AL, Y, caches, lambd, m, classification, keep_probs)\u001b[0m\n\u001b[0;32m     38\u001b[0m                                                                                                            \u001b[0mlambd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                                                                                                            \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                                                                                                            keep_probs = keep_probs)\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-59-0f932a22e56d>\u001b[0m in \u001b[0;36mlact_backward\u001b[1;34m(dA, cache, lambd, m, activation, keep_probs)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mdZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mdA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_cache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreverse_dropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mL_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-44e07d17a27a>\u001b[0m in \u001b[0;36mreverse_dropout\u001b[1;34m(dW, D, keep_probs)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreverse_dropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeep_probs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mavg_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (25,12288) (25,32) "
     ]
    }
   ],
   "source": [
    "p = L_model(X_train, Y_train, X_test, Y_test, mini_batch_size = 32, learning_rate = 0.0001, epoch = 3000, \n",
    "            layer_dims = [12288, 25, 12, 6], \n",
    "            classification = 'softmax', lambd = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_p = init_params_he([10, 7, 4, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.79989897,  0.19521314,  0.04315498, -0.83337927, -0.12405178,\n",
       "         -0.15865304, -0.03700312, -0.28040323, -0.01959608, -0.21341839],\n",
       "        [-0.58757818,  0.39561516,  0.39413741,  0.76454432,  0.02237573,\n",
       "         -0.18097724, -0.24389238, -0.69160568,  0.43932807, -0.49241241],\n",
       "        [-0.52996892, -0.09196943,  0.66462575,  0.10586273, -0.45785063,\n",
       "         -0.31886025,  0.27961805, -0.07178376, -0.34383407, -0.10287287],\n",
       "        [ 0.33319929,  0.88374361, -0.55638887, -0.28014216, -0.35945513,\n",
       "         -1.08184688, -0.41313235, -0.45789116,  0.50265822, -0.05899384],\n",
       "        [-0.72595532,  0.28920205, -0.15932913, -0.77955637, -0.26682983,\n",
       "         -0.26322741, -0.39081204,  0.01328842, -1.00545144, -0.11974675],\n",
       "        [ 0.45310941,  0.38138279,  0.49559652,  0.50060672,  0.66524951,\n",
       "         -0.50011927,  0.3782682 , -0.8322151 , -0.26961842, -0.85617793],\n",
       "        [ 0.46874582,  0.59646569, -0.08828653,  0.79364539, -0.30174732,\n",
       "          0.06735791,  0.0683994 , -0.47592259,  0.19585568,  0.86713753]]),\n",
       " 'W2': array([[-0.5478486 ,  0.48071662, -0.08258739,  0.94590558,  0.25859575,\n",
       "          0.36145287,  0.34378523],\n",
       "        [ 0.13314245, -0.74606697,  0.74387511, -0.73265341,  0.12751739,\n",
       "          0.32823801, -0.44788295],\n",
       "        [ 0.07753955,  0.62425934, -0.01288438, -0.47500737, -1.55852739,\n",
       "         -0.5194706 , -0.31594488],\n",
       "        [-0.27603669, -0.51313954,  0.20167279, -0.30719457, -0.0585058 ,\n",
       "          0.36297904, -0.4572504 ]]),\n",
       " 'W3': array([[-0.21227775,  1.52604203,  0.61821336, -0.91466852],\n",
       "        [-0.05638536,  0.39915154,  0.87219574,  0.10534929]]),\n",
       " 'b1': array([[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]]),\n",
       " 'b2': array([[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]]),\n",
       " 'b3': array([[ 0.],\n",
       "        [ 0.]])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_x = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
