{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-Layers Generalized Neural Network\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Import/Prep Test Dataset\n",
    "2. Create Activation functions (Forward and Backward)\n",
    "3. Individual Functions\n",
    "    - Initializing Parameters\n",
    "    - Forward Propogation\n",
    "    - Compute Cost\n",
    "    - Backward Propogation\n",
    "    - Gradients\n",
    "    - Update Parameters\n",
    "9. Step-by-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import/Prep Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 64, 64, 3)\n",
      "(1, 209)\n",
      "(50, 64, 64, 3)\n",
      "(1, 50)\n",
      "(1, 209)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "print (train_x_orig.shape)\n",
    "print (train_y.shape)\n",
    "print (test_x_orig.shape)\n",
    "print (test_y.shape)\n",
    "print (train_y.shape)\n",
    "print (classes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten the training and testing input sets and normalize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 209)\n",
      "(12288, 50)\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x_orig.reshape(train_x_orig.shape[0],-1).T/255\n",
    "test_x = test_x_orig.reshape(test_x_orig.shape[0],-1).T/255\n",
    "print (train_x.shape)\n",
    "print (test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x)), x\n",
    "sigmoid(3)\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    s = 1/(1+np.exp(-cache))\n",
    "    return dA*s*(1-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x), x\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z<=0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_dims = [12288, 20, 7, 5, 1]\n",
    "def par_deep(layer_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1,L):\n",
    "        parameters.update({\n",
    "                            'W{0}'.format(l) : np.random.randn(layer_dims[l], layer_dims[l-1])/np.sqrt(layer_dims[l-1]),\n",
    "                            'b{0}'.format(l) : np.zeros((layer_dims[l], 1))\n",
    "                          })\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dims = [12288, 20, 7, 5, 1]\n",
    "[l for l in range(1,len(layer_dims))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Forward Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_forward(A, W, b):\n",
    "    return W.dot(A)+b, (A,W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lact_forward(A_prev, W, b, activation = 'sigmoid'):\n",
    "    Z, l_cache = l_forward(A_prev, W, b)\n",
    "    if activation == 'relu':\n",
    "        A, a_cache = relu(Z)\n",
    "        return A, (l_cache, a_cache)\n",
    "    A, a_cache = sigmoid(Z)\n",
    "    return A, (l_cache, a_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters)//2\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = lact_forward(A_prev, parameters['W{0}'.format(l)], parameters['b{0}'.format(l)], 'relu')\n",
    "        caches.append(cache)\n",
    "    AL, cache = lact_forward(A, parameters[\"W{0}\".format(L)], parameters[\"b{0}\".format(L)], 'sigmoid')\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(AL, Y):\n",
    "    return np.squeeze((1./Y.shape[1])*(-np.dot(Y, np.log(AL).T)- np.dot(1-Y, np.log(1-AL).T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Backward Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1./m * np.dot(dZ, A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lact_backward(dA, cache, activation = 'relu'):\n",
    "    l_cache, a_cache = cache\n",
    "    if activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA, a_cache)\n",
    "    elif activation == 'relu':\n",
    "        dZ = relu_backward(dA, a_cache)\n",
    "    dA_prev, dW, db = l_backward(dZ, l_cache)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    dAL = - (np.divide(Y,AL) - np.divide(1-Y, 1-AL))\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA{0}\".format(L)], grads['dW{0}'.format(L)], grads['db{0}'.format(L)] = lact_backward(dAL, current_cache, 'sigmoid')\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        grads['dA{0}'.format(l+1)], grads['dW{0}'.format(l+1)], grads['db{0}'.format(l+1)] = lact_backward(grads['dA{0}'.format(l+2)], \n",
    "                                                                                                     current_cache)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_par(parameters, grads, learning_rate):\n",
    "    L = len(parameters)//2\n",
    "    for l in range(L):\n",
    "        parameters['W{0}'.format(l+1)] = parameters['W{0}'.format(l+1)] - learning_rate*grads['dW{0}'.format(l+1)]\n",
    "        parameters['b{0}'.format(l+1)] = parameters['b{0}'.format(l+1)] - learning_rate*grads['db{0}'.format(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    probas, cache = L_forward(X, parameters)\n",
    "    p = np.vectorize(lambda x:1 if x>0.5 else 0)(probas)\n",
    "    accuracy = np.sum(np.vectorize(lambda x:1 if x==0 else 0)(p-y), axis=1)/y.shape[1]\n",
    "    #print(\"Accuracy: \"  + str(accuracy))\n",
    "    return accuracy, p, probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12288, 20, 7, 5, 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.01465338, -0.00551871, -0.00476469, ..., -0.00475605,\n",
      "        -0.00343108,  0.00856474],\n",
      "       [ 0.00910437,  0.00207385, -0.0059909 , ...,  0.00622328,\n",
      "        -0.0044052 ,  0.00187287],\n",
      "       [-0.00321458, -0.00176345,  0.00574466, ...,  0.00742212,\n",
      "        -0.00094203, -0.00593549],\n",
      "       ..., \n",
      "       [ 0.00157639, -0.0011742 ,  0.01656117, ..., -0.00832292,\n",
      "        -0.00744053, -0.00138343],\n",
      "       [-0.00031528, -0.00376196, -0.00771457, ...,  0.02027339,\n",
      "        -0.00481399, -0.00026661],\n",
      "       [-0.01293895, -0.0100192 ,  0.00655218, ...,  0.01983405,\n",
      "         0.01387618,  0.00673519]]), 'b1': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]]), 'W2': array([[ 0.20259018, -0.3085417 , -0.37746612, -0.04024358,  0.05940078,\n",
      "         0.12334556,  0.30837079,  0.04762441,  0.05302836,  0.13488878,\n",
      "         0.21657182, -0.63424146, -0.15595979,  0.10192011, -0.14986769,\n",
      "         0.05316226,  0.11436514,  0.42089156,  0.10855153, -0.21844142],\n",
      "       [ 0.24292279, -0.09271047,  0.24633566,  0.2407009 , -0.07419339,\n",
      "        -0.3140973 , -0.2378063 ,  0.38587265, -0.06125378,  0.36687191,\n",
      "        -0.03075124,  0.05074813,  0.00091604,  0.30299404,  0.10224218,\n",
      "        -0.50638283, -0.12615196,  0.17816799,  0.12788617, -0.05048083],\n",
      "       [ 0.24702443,  0.18465938, -0.08472851,  0.00682211,  0.26301448,\n",
      "        -0.16793632, -0.02160583, -0.05880307, -0.18442688, -0.08419676,\n",
      "        -0.49760503,  0.04897579,  0.05342117,  0.29123563, -0.00186282,\n",
      "        -0.06889612, -0.21222632,  0.13600029,  0.39350563, -0.15920307],\n",
      "       [ 0.13283642, -0.25233864,  0.38848562,  0.0175009 , -0.05934642,\n",
      "        -0.03763294, -0.16316537, -0.1598778 ,  0.00866664, -0.12812524,\n",
      "         0.38642985, -0.34200394,  0.00209062, -0.3747224 , -0.10733094,\n",
      "         0.15272388,  0.31735402,  0.18825871, -0.04812246, -0.12164815],\n",
      "       [-0.16524061,  0.39520346, -0.58172048,  0.15473167, -0.31326215,\n",
      "         0.03372203, -0.26115673,  0.26811862,  0.47707542, -0.1768339 ,\n",
      "        -0.14139523, -0.07722686,  0.00301682, -0.28733558,  0.0486702 ,\n",
      "        -0.07638937,  0.14664864,  0.07824474, -0.14710693, -0.3941019 ],\n",
      "       [-0.2435609 ,  0.09695246,  0.17821347,  0.20294394,  0.15516715,\n",
      "         0.26508366, -0.02808966, -0.2858965 ,  0.33423678,  0.05018149,\n",
      "        -0.23354711,  0.05059482,  0.01754097, -0.36721984,  0.16682193,\n",
      "        -0.35173   , -0.06485109, -0.30370136,  0.12028256, -0.3798758 ],\n",
      "       [ 0.20688506, -0.23459053, -0.40563652,  0.05946033,  0.0917243 ,\n",
      "        -0.32768127, -0.06881328, -0.61338784,  0.19020316, -0.1425967 ,\n",
      "         0.00564071, -0.13178665,  0.0934473 , -0.02009862,  0.09349342,\n",
      "         0.38545417,  0.33119379, -0.01765705,  0.09299516, -0.11263983]]), 'b2': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]]), 'W3': array([[ 0.20930634,  0.36232545,  0.86648532, -1.07247756, -0.63999126,\n",
      "        -0.17415109, -0.15701343],\n",
      "       [-0.57997754,  0.27859535, -0.0550274 , -0.13802435,  0.23410522,\n",
      "         0.75062596, -0.34516171],\n",
      "       [ 0.16010997,  0.29166347,  0.00930796,  0.2653009 , -0.06437531,\n",
      "         0.77601349, -0.01006499],\n",
      "       [ 0.52945384,  0.5420795 ,  0.98032613, -0.05469069,  0.40245915,\n",
      "         0.00502241, -0.01420974],\n",
      "       [-0.96753783, -0.31259346,  0.52514685, -0.18815086, -0.39308334,\n",
      "         0.19497039, -0.58863959]]), 'b3': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]]), 'W4': array([[-0.38423553, -0.01975151,  0.50081053,  0.89766368,  0.01869816]]), 'b4': array([[ 0.]])}\n"
     ]
    }
   ],
   "source": [
    "parameters = par_deep(layer_dims)\n",
    "print (parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Outputs of the first Layer\n",
    "$$W^{[1]T}A^{[0]}+b^{[1]}$$\n",
    "*where*\n",
    "$$A^{[0]} = X$$\n",
    "*Output*\n",
    "$$Z^{[1]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07196659,  0.29407045,  0.52046231, ...,  0.38338442,\n",
       "         0.19791222,  0.15667228],\n",
       "       [-0.43149218, -0.34794355,  0.00957635, ..., -0.0440066 ,\n",
       "         0.17248276, -0.14145979],\n",
       "       [ 0.31849711,  0.11535033,  0.33366745, ...,  0.01670034,\n",
       "         0.09194017,  0.11541118],\n",
       "       ..., \n",
       "       [-0.07870905,  0.03190587, -0.01356782, ...,  0.18634333,\n",
       "         0.18820044,  0.17551744],\n",
       "       [-0.47167149, -0.15582351, -0.71908939, ..., -0.60416858,\n",
       "         0.07156221, -0.30375025],\n",
       "       [ 0.22106564,  0.29806307,  0.36277627, ...,  0.58466116,\n",
       "         0.1014699 ,  0.02667976]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_forward(train_x, parameters['W1'], parameters['b1'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*and*\n",
    "$$A^{[0]}, W^{[1]}, b^{[1]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.06666667,  0.76862745,  0.32156863, ...,  0.56078431,\n",
       "          0.08627451,  0.03137255],\n",
       "        [ 0.12156863,  0.75294118,  0.27843137, ...,  0.60784314,\n",
       "          0.09411765,  0.10980392],\n",
       "        [ 0.21960784,  0.74509804,  0.26666667, ...,  0.64705882,\n",
       "          0.09019608,  0.20784314],\n",
       "        ..., \n",
       "        [ 0.        ,  0.32156863,  0.54117647, ...,  0.33333333,\n",
       "          0.01568627,  0.        ],\n",
       "        [ 0.        ,  0.31372549,  0.55294118, ...,  0.41960784,\n",
       "          0.01960784,  0.        ],\n",
       "        [ 0.        ,  0.31764706,  0.55686275, ...,  0.58431373,\n",
       "          0.        ,  0.        ]]),\n",
       " array([[ 0.01465338, -0.00551871, -0.00476469, ..., -0.00475605,\n",
       "         -0.00343108,  0.00856474],\n",
       "        [ 0.00910437,  0.00207385, -0.0059909 , ...,  0.00622328,\n",
       "         -0.0044052 ,  0.00187287],\n",
       "        [-0.00321458, -0.00176345,  0.00574466, ...,  0.00742212,\n",
       "         -0.00094203, -0.00593549],\n",
       "        ..., \n",
       "        [ 0.00157639, -0.0011742 ,  0.01656117, ..., -0.00832292,\n",
       "         -0.00744053, -0.00138343],\n",
       "        [-0.00031528, -0.00376196, -0.00771457, ...,  0.02027339,\n",
       "         -0.00481399, -0.00026661],\n",
       "        [-0.01293895, -0.0100192 ,  0.00655218, ...,  0.01983405,\n",
       "          0.01387618,  0.00673519]]),\n",
       " array([[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_forward(train_x, parameters['W1'], parameters['b1'])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Activations of the First Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Output of the first layer, activated with RELU (**RE**ctified **L**inear **U**nit)\n",
    "$$relu(W^{[l]T}A^{[l-1]}b^{[l]})$$\n",
    "**Output**\n",
    "$$A^{[l]}, ((A^{[l-1]}, W^{[l]}, b^{[l]}), Z^{[l]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: $$A^{[1]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07196659,  0.29407045,  0.52046231, ...,  0.38338442,\n",
       "         0.19791222,  0.15667228],\n",
       "       [ 0.        ,  0.        ,  0.00957635, ...,  0.        ,\n",
       "         0.17248276,  0.        ],\n",
       "       [ 0.31849711,  0.11535033,  0.33366745, ...,  0.01670034,\n",
       "         0.09194017,  0.11541118],\n",
       "       ..., \n",
       "       [ 0.        ,  0.03190587,  0.        , ...,  0.18634333,\n",
       "         0.18820044,  0.17551744],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.07156221,  0.        ],\n",
       "       [ 0.22106564,  0.29806307,  0.36277627, ...,  0.58466116,\n",
       "         0.1014699 ,  0.02667976]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lact_forward(train_x, parameters['W1'], parameters['b1'], 'relu')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Forward Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All L-1 layers are activated using RELU while the final layer(outcome) is activated using sigmoid(binary classification).\n",
    "*Output*\n",
    "$$A^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL, cache = L_forward(train_x, parameters)\n",
    "len(cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.7717493284237686)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(AL, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative outputs of the last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `cache` element houses...\n",
    "$$A^{[l]}, ((A^{[l-1]}, W^{[l]}, b^{[l]}), Z^{[l]})$$\n",
    "*where*\n",
    "\n",
    "$A^{[l]}$ : Layer $l$'s activated output\n",
    "\n",
    "$A^{[l-1]}$ : Previous layer's activated output.  $A^{[0]}$ corresponds to the input layer, $X$\n",
    "\n",
    "$W^{[l]}$ : Corresponding weights for the layer $l$\n",
    "\n",
    "$b^{[l]}$ : Biases\n",
    "\n",
    "$Z^{[l]}$ : Linear outputs of layer $l$\n",
    "\n",
    "Let's print out the $Z^{[L]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23682965,  0.3617703 ,  0.4547016 ,  0.11587638,  0.53696866,\n",
       "         0.33363595,  0.22705512,  0.00654325,  0.25821207,  0.18041241,\n",
       "         0.44291608,  0.47733644,  0.4013233 ,  0.56109576,  0.36690789,\n",
       "         0.85804303,  0.36050484,  0.73619789,  0.41969672,  0.29960153,\n",
       "         0.42308701,  0.30968265,  0.59063732,  0.66340383,  0.66475275,\n",
       "         0.13592181,  0.4935429 ,  0.3435607 ,  0.32336825,  0.39839019,\n",
       "         0.28137059,  0.65873112,  0.33357562,  0.69655044,  0.37610349,\n",
       "         0.21004222,  0.23689138,  0.21534942,  0.07174055,  0.48913821,\n",
       "         0.45587072,  0.31859619,  0.56083865,  0.50423751,  0.2747355 ,\n",
       "         0.21311506,  0.59085843,  0.3384595 ,  0.41909391,  0.30677565,\n",
       "         0.6126749 ,  0.35049577,  0.44611779,  0.48544772,  0.27416875,\n",
       "         0.50024744,  0.31504915,  0.61115568,  0.175417  ,  0.19048603,\n",
       "         0.23223954,  0.36397181,  0.38315242,  0.30036867,  0.46848748,\n",
       "         0.22486395,  0.46274062,  0.18505195,  0.40902872,  0.38770436,\n",
       "         0.51071743,  0.14255054,  0.16874203,  0.37225644,  0.65405564,\n",
       "         0.46838984,  0.26272078,  0.18777017,  0.31485579,  0.11567342,\n",
       "         0.41370585,  0.56475231,  0.26180264,  0.19286887,  0.58820316,\n",
       "         0.31117293,  0.38919682,  0.09487172,  0.45009003,  0.39109174,\n",
       "         0.38773551,  0.42925431,  0.25772982,  0.29285487,  0.44620762,\n",
       "         0.52818201,  0.50850823,  0.59451617,  0.5530887 ,  0.30604571,\n",
       "         0.38720736,  0.29848702,  0.394368  ,  0.24884781,  0.17890348,\n",
       "         0.34942324,  0.36566934,  0.29267988,  0.30519431,  0.39725812,\n",
       "         0.53758023,  0.45149286,  0.29749453,  0.51514816,  0.45394744,\n",
       "         0.55192286,  0.540798  ,  0.38506009,  0.05341113,  0.28455102,\n",
       "         0.34419852,  0.51479599,  0.26142473,  0.49309355,  0.25908332,\n",
       "         0.3970278 ,  0.24971558,  0.29119102,  0.45539544,  0.18160673,\n",
       "         0.360563  ,  0.42393187,  0.38333522,  0.24098787,  0.24069896,\n",
       "         0.48462412,  0.58616387,  0.48762207,  0.21900517,  0.19434446,\n",
       "         0.44225226,  0.50096568,  0.33567203,  0.27032806,  0.44702187,\n",
       "         0.26416864,  0.50534372,  0.55459995,  0.21046047,  0.41263766,\n",
       "         0.42233406,  0.28752326,  0.50809418,  0.14543675,  0.1647728 ,\n",
       "         0.27312263,  0.34464258,  0.14949107,  0.55767333,  0.30262092,\n",
       "         0.23219747,  0.39394367,  0.45931018,  0.33904088,  0.46407021,\n",
       "         0.16556036,  0.33871897,  0.3257718 ,  0.4416686 ,  0.61987985,\n",
       "         0.27020269,  0.56129575,  0.53872252,  0.43133397,  0.4444239 ,\n",
       "         0.34804691,  0.32898886,  0.27736076,  0.45600448,  0.32906039,\n",
       "         0.12541511,  0.08058399,  0.53764821,  0.48020828,  0.25541906,\n",
       "         0.2271079 ,  0.35241253,  0.35203764,  0.21353985,  0.32631195,\n",
       "         0.54249165,  0.57814502,  0.60566084,  0.41701202,  0.45236746,\n",
       "         0.01651586,  0.44230629,  0.23876631,  0.14138988,  0.42928843,\n",
       "         0.38240626,  0.67527317,  0.23946393,  0.62017711,  0.66551924,\n",
       "         0.11912908,  0.26001171,  0.16628344,  0.18452023]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l, a = cache[-1] # the last layer\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is the final layer, we cannot derive the $dAL$ from the next layer since there is \n",
    "For this layer, we used a $\\sigma(x)$ activation, so we must use the derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.26722523,  2.43586908, -1.63463731,  2.12285706,  2.71081293,\n",
       "         2.39603482,  2.25489904, -1.99347811,  2.29461334,  2.19771121,\n",
       "         2.55724164, -1.62043375,  2.49380014, -1.5705835 , -1.69287346,\n",
       "         3.35854057,  2.4340532 ,  3.08798167,  2.52150004, -1.74111348,\n",
       "         2.52666712,  2.3629925 ,  2.80513851,  2.94138927, -1.5144007 ,\n",
       "        -1.87291088,  2.63810962, -1.70924043,  2.3817741 , -1.6714    ,\n",
       "         2.32494452,  2.93233887,  2.3959506 ,  3.00681811,  2.45659787,\n",
       "         2.23373015,  2.26730345,  2.2402952 , -1.93077235,  2.63091011,\n",
       "         2.57754639, -1.72716913, -1.57073022,  2.65572256,  2.3161825 ,\n",
       "         2.23752704,  2.80553768, -1.71286765,  2.52058314,  2.35903604,\n",
       "        -1.5418994 ,  2.41977125,  2.56223547,  2.62490235, -1.76020378,\n",
       "         2.64912928, -1.72975301, -1.54272329,  2.19174307, -1.82655731,\n",
       "        -1.7927562 , -1.69491078,  2.4669016 ,  2.35035655,  2.597576  ,\n",
       "         2.25215235,  2.58842129,  2.20328094, -1.66429515,  2.47359407,\n",
       "         2.66648636, -1.86714373,  2.18381471,  2.45100503,  2.92332534,\n",
       "         2.59742002,  2.30046355,  2.20655618,  2.37006173,  2.12262918,\n",
       "         2.51241219,  2.75901205,  2.2992701 , -1.82459009, -1.55532422,\n",
       "         2.36502526,  2.47579499,  2.0995178 , -1.63757075,  2.47859416,\n",
       "         2.47363997,  2.53611163, -1.772804  , -1.74613042, -1.64005087,\n",
       "         2.69584647,  2.66280882, -1.55182949,  2.7386148 ,  2.35804437,\n",
       "         2.47286188,  2.34781804, -1.67410594,  2.28254683, -1.8361866 ,\n",
       "         2.41824932, -1.69373215, -1.746261  , -1.73698016, -1.67216051,\n",
       "        -1.58416008, -1.63667697,  2.34648101,  2.67388649,  2.57451524,\n",
       "         2.73658903,  2.71737678, -1.68040975,  2.05486324,  2.32916513,\n",
       "         2.41085869, -1.5976225 ,  2.29877918,  2.6373737 , -1.77175872,\n",
       "         2.48739728, -1.77902232,  2.33802015, -1.63419713, -1.83392923,\n",
       "         2.4341366 ,  2.5279575 ,  2.46716977, -1.78585116, -1.78607823,\n",
       "        -1.61592867, -1.55645784, -1.61408491,  2.24483771,  2.21451457,\n",
       "         2.55620826,  2.65031418, -1.71485752,  2.31039427, -1.63952992,\n",
       "        -1.76784404, -1.60329817,  2.74124426, -1.81021108, -1.66190207,\n",
       "         2.52551806,  2.3331216 ,  2.66212047, -1.86464458,  2.17912519,\n",
       "         2.31406138, -1.70847353,  2.1612431 ,  2.74660401,  2.35340131,\n",
       "         2.26136879,  2.48281702, -1.63171927,  2.40360072, -1.62871941,\n",
       "         2.18005419, -1.71268271,  2.38509925,  2.55530022, -1.53800908,\n",
       "        -1.76322478, -1.5704694 ,  2.7138161 ,  2.53930955, -1.64119356,\n",
       "        -1.70606575,  2.38956237, -1.75778107,  2.57775741, -1.71959956,\n",
       "         2.13361893,  2.08391988,  2.71197591,  2.61641104,  2.29100252,\n",
       "        -1.7968348 ,  2.42249523,  2.42196204, -1.80771997,  2.38584762,\n",
       "         2.72028789,  2.78272843, -1.54571368,  2.51742076,  2.5720295 ,\n",
       "         2.016653  ,  2.55629235, -1.78759891,  2.15187366,  2.53616405,\n",
       "        -1.68221784,  2.96456956,  2.27056786,  2.8592573 ,  2.94550045,\n",
       "         2.12651531,  2.29694527,  2.18090777,  2.20264131]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAL = - (np.divide(train_y, AL)-np.divide(1-train_y, 1-AL))\n",
    "dAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sigmoid_backward`, or derivative of the sigmoid function, will take a the $dAL$ and the activation_cache, `a`, which is also the $Z^{[L]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.55893222,  0.58946891, -0.3882435 ,  0.52893672,  0.63110697,\n",
       "         0.58264379,  0.55652116, -0.49836419,  0.56419673,  0.54498116,\n",
       "         0.60895365, -0.38288128,  0.59900556, -0.36329396, -0.40928839,\n",
       "         0.70225162,  0.58916264,  0.67616388,  0.60341067, -0.4256549 ,\n",
       "         0.60422171,  0.5768078 ,  0.64351136,  0.6600246 , -0.33967278,\n",
       "        -0.46607176,  0.62094069, -0.4149448 ,  0.5801449 , -0.40169918,\n",
       "         0.56988221,  0.65897529,  0.58262912,  0.66742252,  0.59293297,\n",
       "         0.55231835,  0.55894744,  0.55363025, -0.48207255,  0.6199034 ,\n",
       "         0.61203414, -0.4210179 , -0.36335343,  0.62345464,  0.56825509,\n",
       "         0.55307803,  0.64356209, -0.41618373,  0.60326641,  0.57609804,\n",
       "        -0.35144926,  0.5867378 ,  0.60971581,  0.61903345, -0.43188396,\n",
       "         0.62251748, -0.42188278, -0.35179562,  0.54374214, -0.45252197,\n",
       "        -0.44219967, -0.40999844,  0.5946332 ,  0.57453264,  0.6150257 ,\n",
       "         0.5559803 ,  0.61366413,  0.54613142, -0.39914504,  0.59572995,\n",
       "         0.62497464, -0.46442259,  0.54208569,  0.5920041 ,  0.65792381,\n",
       "         0.61500258,  0.565305  ,  0.5468051 ,  0.57807006,  0.52888615,\n",
       "         0.60197614,  0.63755142,  0.56507937, -0.45193169, -0.35704724,\n",
       "         0.57717153,  0.59608934,  0.52370016, -0.38933936,  0.59654549,\n",
       "         0.59573745,  0.60569559, -0.43592185, -0.42730509, -0.39026282,\n",
       "         0.62905899,  0.6244567 , -0.35559931,  0.6348519 ,  0.57591977,\n",
       "         0.59561025,  0.57407261, -0.40266624,  0.56189289, -0.45539304,\n",
       "         0.58647771, -0.40958788, -0.42734792, -0.42428818, -0.40197128,\n",
       "        -0.36875066, -0.38900588,  0.57382992,  0.62601255,  0.61157736,\n",
       "         0.63458159,  0.63199803, -0.40490705,  0.51334961,  0.57066161,\n",
       "         0.58521003, -0.37406991,  0.56498649,  0.62083492, -0.43558906,\n",
       "         0.59797335, -0.43789351,  0.57228769, -0.38807872, -0.45472269,\n",
       "         0.58917671,  0.60442373,  0.59467726, -0.44004292, -0.44011411,\n",
       "        -0.3811608 , -0.35751552, -0.38045391,  0.5545335 ,  0.54843377,\n",
       "         0.60879557,  0.62268624, -0.41686118,  0.56717344, -0.39006908,\n",
       "        -0.43433924, -0.3762857 ,  0.63520215, -0.44757824, -0.39827983,\n",
       "         0.60404163,  0.57138968,  0.6243596 , -0.46370477,  0.54110025,\n",
       "         0.56785935, -0.41468218,  0.53730332,  0.63591402,  0.5750831 ,\n",
       "         0.55778995,  0.5972317 , -0.38714948,  0.58395752, -0.3860207 ,\n",
       "         0.54129581, -0.41612069,  0.58073024,  0.60865655, -0.34980878,\n",
       "        -0.43285734, -0.3632477 ,  0.63151519,  0.60619216, -0.39068735,\n",
       "        -0.41385612,  0.58151333, -0.43110094,  0.6120659 , -0.41846926,\n",
       "         0.53131274,  0.5201351 ,  0.63126516,  0.61779706,  0.56350986,\n",
       "        -0.44346581,  0.58720249,  0.58711161, -0.44681698,  0.58086175,\n",
       "         0.63239185,  0.64064046, -0.35304965,  0.60276803,  0.61120197,\n",
       "         0.50412887,  0.60880844, -0.4405904 ,  0.5352887 ,  0.60570374,\n",
       "        -0.40554667,  0.6626829 ,  0.55958154,  0.65025883,  0.66049912,\n",
       "         0.5297471 ,  0.56463917,  0.54147534,  0.54599962]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_backward(dAL, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of sigmoid_backward is then inserted to the linear_backward function with the linear cache which gives you the...\n",
    "$$dA^{[L-1]}, dW^{[L]}, db^{[L]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example : \n",
    "$$dA^{[L-1]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21476162, -0.2264949 ,  0.14917695, ..., -0.21695443,\n",
       "        -0.20805406, -0.20979245],\n",
       "       [-0.01103975, -0.0116429 ,  0.00766839, ..., -0.01115248,\n",
       "        -0.01069495, -0.01078432],\n",
       "       [ 0.27991914,  0.29521224, -0.19443643, ...,  0.28277724,\n",
       "         0.27117655,  0.27344236],\n",
       "       [ 0.50173315,  0.52914482, -0.34851209, ...,  0.50685607,\n",
       "         0.48606274,  0.49012402],\n",
       "       [ 0.01045101,  0.01102199, -0.00725944, ...,  0.01055772,\n",
       "         0.01012459,  0.01020919]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dA_prev, dW, db = l_backward(sigmoid_backward(dAL, a), l)\n",
    "dA_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...The gradients for the current layer, $dW^{[L]}, db^{[L]}$, and the derivative of the activation for the previous layer, $dA^{[L-1]}$, which is used in conjunction with the activation_cache for that layer then inserted in to the proper backward_activation function, which in this case, all $L-1$ (in between) layers are activated with RELU.  The result is then $dZ^{[L-1]}$, and the cycle continues until we reach the first layer. \n",
    "\n",
    "**Note** that this entire process happens nested inside the linear_activation_backward functions which performs the specific backward_activation calculation which gets put inside the linear_backward function.  That function is called the `lact_backward`.  Below, the equivalency and convenience is shown printing the $dW^{[L]}$ using both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02203238  0.00799533  0.05356974  0.08891859  0.        ]]\n",
      "[[ 0.02203238  0.00799533  0.05356974  0.08891859  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(lact_backward(dAL, cache[-1], 'sigmoid')[1])\n",
    "print(l_backward(sigmoid_backward(dAL, a), l)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the full backward propogation to all the layers.  This results in a dictionary of gradients.  Our `layer_dims` was `[12288, 20, 7, 5, 1]`, so we have 3 hidden-layers and 1 output-layer and 1 input-layer.  We should have the following gradients for the hidden layers.\n",
    "\\begin{bmatrix}\n",
    "dA^{[4]} & dA^{[3]} & dA^{[2]} & dA^{[1]} \\\\\n",
    "dW^{[4]} & dW^{[3]} & dW^{[2]} & dW^{[1]} \\\\\n",
    "db^{[4]} & db^{[3]} & db^{[2]} & db^{[1]}\n",
    "\\end{bmatrix}\n",
    "\n",
    "**Note** $dA^{[4]}$ is equivalent to the last layer, $dA^{[L]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dA4', 'dW4', 'db4', 'dA3', 'dW3', 'db3', 'dA2', 'dW2', 'db2', 'dA1', 'dW1', 'db1'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = L_backward(AL, train_y, cache)\n",
    "g.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example : \n",
    "$$ dA^{[4]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21476162, -0.2264949 ,  0.14917695, ..., -0.21695443,\n",
       "        -0.20805406, -0.20979245],\n",
       "       [-0.01103975, -0.0116429 ,  0.00766839, ..., -0.01115248,\n",
       "        -0.01069495, -0.01078432],\n",
       "       [ 0.27991914,  0.29521224, -0.19443643, ...,  0.28277724,\n",
       "         0.27117655,  0.27344236],\n",
       "       [ 0.50173315,  0.52914482, -0.34851209, ...,  0.50685607,\n",
       "         0.48606274,  0.49012402],\n",
       "       [ 0.01045101,  0.01102199, -0.00725944, ...,  0.01055772,\n",
       "         0.01012459,  0.01020919]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g['dA4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the `update_parameters` function to iteratively update our old parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old W1 was\n",
      "[[ 0.01443107 -0.00578819 -0.00498028 ..., -0.00496957 -0.00367704\n",
      "   0.008408  ]\n",
      " [ 0.00923991  0.00223314 -0.00587055 ...,  0.00635394 -0.00425329\n",
      "   0.0019733 ]\n",
      " [-0.00326254 -0.00181919  0.00569773 ...,  0.00733889 -0.00102616\n",
      "  -0.00599855]\n",
      " ..., \n",
      " [ 0.00135389 -0.00144156  0.0163466  ..., -0.00851114 -0.0076693\n",
      "  -0.0015267 ]\n",
      " [-0.00032176 -0.00376954 -0.00772468 ...,  0.02027105 -0.00481998\n",
      "  -0.00027055]\n",
      " [-0.01280684 -0.00985645  0.00668177 ...,  0.01995729  0.01401994\n",
      "   0.00683061]]\n",
      "Now it is...\n",
      "[[ 0.01420875 -0.00605767 -0.00519586 ..., -0.00518309 -0.00392299\n",
      "   0.00825126]\n",
      " [ 0.00937544  0.00239243 -0.00575019 ...,  0.0064846  -0.00410139\n",
      "   0.00207374]\n",
      " [-0.00331051 -0.00187493  0.00565079 ...,  0.00725566 -0.00111028\n",
      "  -0.0060616 ]\n",
      " ..., \n",
      " [ 0.00113138 -0.00170892  0.01613202 ..., -0.00869936 -0.00789808\n",
      "  -0.00166996]\n",
      " [-0.00032824 -0.00377712 -0.00773478 ...,  0.02026871 -0.00482596\n",
      "  -0.00027449]\n",
      " [-0.01267472 -0.00969371  0.00681137 ...,  0.02008054  0.0141637\n",
      "   0.00692603]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Old W1 was\")\n",
    "print (parameters['W1'])\n",
    "print (\"Now it is...\")\n",
    "print(update_par(parameters, g, 0.01)['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model(X, Y, layer_dims, learning_rate = 0.0075, iterations = 3000, print_cost=False, plot=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    ac = []\n",
    "    parameters = par_deep(layer_dims)\n",
    "    for i in range(iterations):\n",
    "        AL, caches = L_forward(X, parameters)\n",
    "        c = cost(AL, Y)\n",
    "        grads = L_backward(AL, Y, caches)\n",
    "        parameters = update_par(parameters, grads, learning_rate)\n",
    "        if print_cost and i%100==0:\n",
    "            print (\"Cost after iteration {0}: {1}\".format(i, c))\n",
    "            costs.append(c)\n",
    "            accuracy = predict(X, Y, parameters)[0]\n",
    "            print (accuracy)\n",
    "            ac.append(accuracy)\n",
    "    # plot the cost\n",
    "    if plot:\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        plt.plot(ac)\n",
    "        plt.ylabel('Accuracy Score')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6812016076456814\n",
      "[ 0.65550239]\n",
      "Cost after iteration 100: 0.5822890145429783\n",
      "[ 0.65550239]\n",
      "Cost after iteration 200: 0.5177450012151461\n",
      "[ 0.65550239]\n",
      "Cost after iteration 300: 0.42934243552094764\n",
      "[ 0.76555024]\n",
      "Cost after iteration 400: 0.44323696356161496\n",
      "[ 0.70813397]\n",
      "Cost after iteration 500: 0.5379355986044362\n",
      "[ 0.93779904]\n",
      "Cost after iteration 600: 0.27654314252570855\n",
      "[ 0.89952153]\n",
      "Cost after iteration 700: 0.24328195786987822\n",
      "[ 0.91866029]\n",
      "Cost after iteration 800: 0.31925010023564165\n",
      "[ 0.93779904]\n",
      "Cost after iteration 900: 0.179015662541538\n",
      "[ 0.96650718]\n",
      "Cost after iteration 1000: 0.15177952063998984\n",
      "[ 0.9569378]\n",
      "Cost after iteration 1100: 0.44147412354313653\n",
      "[ 0.84210526]\n",
      "Cost after iteration 1200: 0.6889495955461816\n",
      "[ 0.53588517]\n",
      "Cost after iteration 1300: 0.5167874680985126\n",
      "[ 0.86124402]\n",
      "Cost after iteration 1400: 0.4517956917837608\n",
      "[ 0.89952153]\n",
      "Cost after iteration 1500: 0.3802982242515906\n",
      "[ 0.89952153]\n",
      "Cost after iteration 1600: 0.6269628155292167\n",
      "[ 0.76555024]\n",
      "Cost after iteration 1700: 0.08275492478316644\n",
      "[ 0.99521531]\n",
      "Cost after iteration 1800: 0.014878090427905532\n",
      "[ 1.]\n",
      "Cost after iteration 1900: 0.008445314906374025\n",
      "[ 1.]\n",
      "Cost after iteration 2000: 0.004991133884650245\n",
      "[ 1.]\n",
      "Cost after iteration 2100: 0.003631510512423099\n",
      "[ 1.]\n",
      "Cost after iteration 2200: 0.0028263251949442567\n",
      "[ 1.]\n",
      "Cost after iteration 2300: 0.00231189845792717\n",
      "[ 1.]\n",
      "Cost after iteration 2400: 0.001956218120082384\n",
      "[ 1.]\n",
      "Cost after iteration 2500: 0.0016985005113059246\n",
      "[ 1.]\n",
      "Cost after iteration 2600: 0.0015012688465295574\n",
      "[ 1.]\n",
      "Cost after iteration 2700: 0.001343441440039408\n",
      "[ 1.]\n",
      "Cost after iteration 2800: 0.0012137743049571927\n",
      "[ 1.]\n",
      "Cost after iteration 2900: 0.0011050433984833112\n",
      "[ 1.]\n"
     ]
    }
   ],
   "source": [
    "p = L_model(train_x, train_y, \n",
    "            learning_rate = 0.0075, \n",
    "            layer_dims = [12288,25, 20, 15, 10, 5, 1], \n",
    "            iterations = 3000, \n",
    "            print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[104]\n",
      " [ 57]\n",
      " [105]]\n",
      "[[0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1\n",
      "  1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0\n",
      "  0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1\n",
      "  0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1\n",
      "  1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0\n",
      "  0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0\n",
      "  1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0\n",
      "  1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0\n",
      "  1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0]\n",
      " [1 0 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0\n",
      "  0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
      "  1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0\n",
      "  1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 0\n",
      "  0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1\n",
      "  1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1]]\n",
      "Cost after iteration 0: [[ 0.69419343  0.69353364  0.69331653]\n",
      " [ 0.67150686  0.67509976  0.67835589]\n",
      " [ 0.69553242  0.69494392  0.69447708]]\n",
      "[ 0.49760766  0.72727273  0.49760766]\n",
      "Cost after iteration 100: [[ 0.66873241  0.83471809  0.7033192 ]\n",
      " [ 0.70709349  0.53716761  0.66064269]\n",
      " [ 0.72137896  0.76948099  0.68653162]]\n",
      "[ 0.54545455  0.72727273  0.52631579]\n",
      "Cost after iteration 200: [[ 0.61863067  1.1677678   0.76859524]\n",
      " [ 0.8816655   0.50706782  0.59210304]\n",
      " [ 0.84096059  0.93160012  0.65627616]]\n",
      "[ 0.58851675  0.72727273  0.77033493]\n",
      "Cost after iteration 300: [[ 0.56011525  1.38811173  0.91755249]\n",
      " [ 1.07155514  0.46827006  0.52627482]\n",
      " [ 1.02822527  0.95707838  0.59587893]]\n",
      "[ 0.68899522  0.72727273  0.73205742]\n",
      "Cost after iteration 400: [[ 0.37695743  1.41148559  1.22957058]\n",
      " [ 1.14543214  0.44514107  0.57653603]\n",
      " [ 1.41400881  0.80505873  0.42074581]]\n",
      "[ 0.8708134   0.72727273  0.89952153]\n",
      "Cost after iteration 500: [[ 0.30259653  1.50891582  1.56592599]\n",
      " [ 1.35704363  0.4072633   0.60728227]\n",
      " [ 1.72139486  0.63971778  0.3262514 ]]\n",
      "[ 0.89952153  0.73205742  0.90909091]\n",
      "Cost after iteration 600: [[ 0.31337149  1.33244738  1.47196429]\n",
      " [ 1.16632539  0.40440419  0.66353362]\n",
      " [ 1.5637694   0.59775893  0.32756036]]\n",
      "[ 0.93779904  0.73684211  0.93779904]\n",
      "Cost after iteration 700: [[ 0.75843694  1.06831799  1.71820452]\n",
      " [ 0.98774471  0.50834742  1.47513111]\n",
      " [ 1.82371558  0.63318883  0.73985241]]\n",
      "[ 0.88995215  0.76555024  0.90909091]\n",
      "Cost after iteration 800: [[ 0.10839863  2.77233873  2.94047562]\n",
      " [ 2.44963681  0.25448262  0.64510661]\n",
      " [ 3.02498976  0.67935735  0.11265138]]\n",
      "[ 0.98564593  0.92344498  0.98564593]\n",
      "Cost after iteration 900: [[ 0.23656692  2.30494782  2.40574537]\n",
      " [ 2.01061902  0.31165361  0.53437957]\n",
      " [ 2.33606886  0.51039468  0.21094004]]\n",
      "[ 0.98564593  0.96172249  0.98564593]\n"
     ]
    }
   ],
   "source": [
    "three_class = np.random.randint(0,2, (3,209))\n",
    "for i in range(three_class.shape[1]):\n",
    "    if three_class[0][i]==1:\n",
    "        three_class[1][i]=0\n",
    "        three_class[2][i]=0\n",
    "    elif three_class[0][i]==0 and three_class[1][i]==0:\n",
    "        three_class[2][i]=1\n",
    "    elif three_class[0][i]==0 and three_class[2][i]==0:\n",
    "        three_class[2][i]=1\n",
    "print (np.apply_over_axes(np.sum, three_class, 1))\n",
    "print (three_class)\n",
    "p = L_model(train_x, three_class, \n",
    "            learning_rate = 0.0075, \n",
    "            layer_dims = [12288,25, 20, 15, 10, 5, 3], \n",
    "            iterations = 1000, \n",
    "            print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.00617291e-03   9.89059023e-01   9.93333977e-01   4.67096752e-02\n",
      "    9.99927192e-01   6.67682515e-02   5.55016020e-02   1.03602499e-02\n",
      "    6.42008343e-03   9.99170315e-01   9.97944155e-01   1.87350354e-01\n",
      "    8.35935659e-02   8.40985961e-03   9.98125490e-01   1.73422762e-02\n",
      "    8.44285016e-03   6.44454513e-03   9.75934064e-01   9.88703868e-01\n",
      "    1.51070846e-02   6.35050889e-02   9.87506679e-01   5.69781502e-03\n",
      "    4.74495677e-02   9.32314123e-02   4.27209221e-02   2.17368680e-03\n",
      "    9.98390543e-01   9.99630838e-01   6.37345148e-02   6.86502369e-03\n",
      "    9.97519982e-01   9.49038644e-01   9.99522703e-01   3.36471724e-02\n",
      "    9.96254559e-01   9.98777577e-01   9.50457753e-01   9.38624176e-01\n",
      "    9.99842209e-01   1.71620694e-02   9.84324204e-03   9.99933675e-01\n",
      "    9.97922630e-01   9.77413493e-01   9.99606238e-01   1.63327009e-02\n",
      "    3.83026711e-03   9.91344536e-01   9.16613633e-01   5.99547110e-03\n",
      "    2.29425087e-02   4.92278269e-01   9.98426842e-01   9.96118786e-01\n",
      "    9.87189670e-01   7.54232650e-04   7.15681772e-02   9.83512936e-01\n",
      "    9.46658904e-01   2.69202378e-02   3.76071900e-02   9.34185483e-01\n",
      "    2.14284244e-03   5.67224208e-02   1.54542388e-04   1.96616364e-01\n",
      "    4.33232711e-02   1.55242926e-02   3.03293837e-03   8.86119278e-01\n",
      "    9.89261210e-01   1.01911987e-02   2.10006253e-03   9.53226915e-01\n",
      "    9.84917027e-01   9.67154144e-01   6.39940889e-02   7.34919582e-01\n",
      "    4.12567250e-03   9.91618743e-01   3.20476362e-02   9.94506397e-01\n",
      "    9.94063345e-01   9.72866252e-01   9.33281379e-01   4.48324719e-02\n",
      "    2.97298950e-02   9.72893121e-01   9.66372850e-01   9.76945818e-01\n",
      "    8.43463850e-01   2.76530773e-02   9.74979999e-01   9.98535234e-01\n",
      "    1.79149517e-02   2.34566353e-02   6.47766440e-03   1.14643406e-01\n",
      "    9.93414046e-01   9.98541676e-01   5.23043398e-02   1.51905946e-02\n",
      "    4.87440571e-02   1.27510706e-02   4.29531197e-03   3.91231437e-02\n",
      "    8.01680265e-01   9.91849973e-01   9.96865472e-01   8.92584950e-02\n",
      "    9.88745448e-01   9.98344981e-01   9.72264096e-01   9.99728474e-01\n",
      "    5.00414333e-03   3.73912590e-03   9.93898805e-01   3.27036757e-02\n",
      "    9.96630918e-01   8.86682147e-01   9.97527053e-01   9.99598706e-01\n",
      "    8.38707540e-04   5.94431862e-03   1.58880270e-02   7.31056578e-01\n",
      "    3.55991319e-03   9.96508771e-01   3.67691534e-01   9.88517527e-01\n",
      "    9.96784100e-01   9.96830724e-01   1.67844102e-01   3.07183159e-01\n",
      "    3.38163963e-03   2.94588097e-04   9.93373433e-01   5.82056934e-03\n",
      "    3.57473887e-03   9.95971333e-01   9.99542478e-01   8.21100235e-01\n",
      "    1.48242600e-02   9.70979676e-01   9.94437869e-01   9.96482145e-01\n",
      "    9.35934605e-01   9.97699300e-01   2.13961804e-01   2.37166892e-02\n",
      "    9.99741472e-01   9.99029825e-01   9.90071235e-01   1.53941006e-02\n",
      "    9.86635471e-01   2.50983649e-02   4.37603142e-03   3.00091550e-02\n",
      "    7.86075792e-01   9.40980455e-03   9.83681730e-01   9.91118956e-01\n",
      "    9.98406412e-01   9.85944913e-01   9.82018547e-01   4.10798778e-02\n",
      "    9.98403998e-01   3.86184681e-03   4.65622328e-02   9.93408822e-01\n",
      "    2.91696613e-02   9.98422952e-01   3.94408389e-02   8.70833267e-02\n",
      "    9.74100772e-01   9.68893026e-01   9.83861312e-01   6.73610857e-03\n",
      "    9.80744454e-01   7.79949407e-03   7.52779349e-02   1.47215198e-02\n",
      "    2.04871065e-02   3.72306702e-02   9.92400748e-01   1.98119202e-02\n",
      "    9.99861973e-01   8.01757206e-01   2.02163551e-03   9.97971418e-01\n",
      "    2.88009185e-03   9.61034682e-01   5.53951370e-03   6.84107054e-02\n",
      "    9.80056156e-01   9.92400426e-01   6.24028203e-02   9.89355313e-01\n",
      "    9.83457655e-01   2.78814644e-03   3.91523656e-02   1.07925670e-01\n",
      "    9.98208460e-01   9.96954517e-01   4.77723050e-02   1.92896411e-01\n",
      "    1.89091256e-02]\n",
      " [  5.22487177e-01   1.04096875e-03   8.70215126e-04   3.18068475e-02\n",
      "    4.38734467e-06   1.98231822e-01   5.59164788e-01   6.42006101e-01\n",
      "    6.99911920e-01   8.10125832e-05   2.85246488e-04   1.15277257e-02\n",
      "    5.17800558e-01   7.02504396e-03   1.12564496e-04   6.14150054e-01\n",
      "    5.60597792e-01   1.03087950e-01   3.99091677e-03   7.65903698e-04\n",
      "    7.26378591e-02   2.80373408e-02   7.98651395e-04   6.56632094e-01\n",
      "    7.90638191e-02   4.88401428e-01   5.62182380e-01   5.18515504e-01\n",
      "    3.41731626e-05   1.33332699e-05   1.42196250e-01   6.92501193e-01\n",
      "    3.32948517e-04   2.23408281e-02   1.68199567e-05   7.26363109e-02\n",
      "    6.23197540e-04   6.78428790e-05   2.48777333e-02   3.20036098e-02\n",
      "    9.48541176e-06   2.86807011e-01   1.94547936e-01   2.96314510e-06\n",
      "    3.19164329e-04   1.69078143e-03   1.64417614e-05   3.18686566e-01\n",
      "    5.19866119e-01   1.58512816e-03   4.86925968e-03   6.28666332e-01\n",
      "    4.74766031e-02   1.00741684e-01   2.56387845e-04   6.41383410e-04\n",
      "    3.55535959e-03   6.11300438e-02   4.17662536e-01   2.09300958e-03\n",
      "    8.56843501e-03   5.10837114e-01   2.02579537e-01   1.85958570e-02\n",
      "    6.99083874e-02   2.41736191e-01   7.42903024e-01   1.65935582e-01\n",
      "    6.00755813e-02   5.66351219e-01   6.31602598e-01   1.56577147e-03\n",
      "    3.53699677e-03   6.02279508e-02   5.89514036e-01   3.81829012e-03\n",
      "    3.14349933e-03   5.47986397e-05   7.04383623e-02   4.60545984e-02\n",
      "    5.79424280e-01   1.18509232e-03   5.78568131e-01   1.33511366e-03\n",
      "    5.57816874e-04   6.52613581e-04   9.75535693e-03   8.77713088e-02\n",
      "    2.91421169e-01   4.15592255e-03   7.38663698e-03   8.97210342e-03\n",
      "    2.28928441e-03   4.57437288e-01   6.83324378e-03   3.30571317e-05\n",
      "    2.20181702e-01   3.41815859e-01   5.48851009e-01   8.94497759e-03\n",
      "    1.21700288e-03   2.01911377e-04   1.45141587e-02   2.95246856e-02\n",
      "    1.06513642e-01   3.85849426e-01   7.23846110e-01   4.97111411e-01\n",
      "    4.95071876e-02   1.12931722e-03   1.88647537e-04   5.82125354e-01\n",
      "    5.93932448e-04   1.49669476e-04   1.04303709e-02   1.86134852e-05\n",
      "    6.64147713e-01   3.38615930e-01   1.40097749e-03   6.86023379e-01\n",
      "    6.62649684e-04   6.37680606e-02   4.20709048e-04   4.07175072e-05\n",
      "    3.68636439e-01   1.27298911e-01   3.10251744e-01   7.02702146e-02\n",
      "    4.29541766e-01   6.87952592e-04   9.59441131e-02   3.65284471e-03\n",
      "    6.48665675e-04   7.03887971e-05   5.76177634e-02   5.79160962e-01\n",
      "    4.40469457e-02   3.19771052e-01   8.00229511e-04   5.05119977e-01\n",
      "    3.55381599e-02   7.12313006e-04   5.00858338e-05   1.18951638e-02\n",
      "    5.72620607e-01   5.43619770e-03   1.29850041e-03   6.78917021e-04\n",
      "    3.28694222e-02   3.56540196e-04   1.03896962e-01   2.53912871e-02\n",
      "    1.33245823e-05   8.47652476e-05   8.99171865e-04   4.12423760e-01\n",
      "    1.10673055e-04   6.42645911e-01   1.24368320e-01   4.08272780e-02\n",
      "    3.55728422e-03   6.25251625e-01   3.68362546e-04   6.78570684e-04\n",
      "    6.68101170e-05   2.97023779e-03   5.53345803e-03   6.58800984e-01\n",
      "    2.29137967e-04   4.43757124e-01   6.00314661e-01   1.94651358e-03\n",
      "    5.91199538e-01   1.93022067e-04   6.61330609e-01   6.11468849e-03\n",
      "    9.88262123e-03   1.34069195e-02   8.74028727e-04   5.50519968e-01\n",
      "    5.83573199e-03   6.89980335e-01   2.15108396e-02   4.39263163e-02\n",
      "    8.87116753e-02   5.72125012e-01   1.88247849e-03   6.14605931e-01\n",
      "    8.85368633e-06   1.58207482e-01   6.22699280e-01   2.58315060e-04\n",
      "    3.64150797e-01   3.17378294e-03   5.74942051e-01   6.06542513e-01\n",
      "    4.94087872e-03   1.67679624e-03   5.50146753e-01   2.68625812e-03\n",
      "    2.94025395e-03   5.89509506e-01   6.92976579e-01   2.79860566e-02\n",
      "    1.26058694e-04   5.82730203e-04   1.12861630e-01   5.17719914e-01\n",
      "    2.59695015e-01]\n",
      " [  9.88905620e-01   1.27816104e-02   7.93043837e-03   9.46917635e-01\n",
      "    1.10876279e-04   9.29869632e-01   9.35922561e-01   9.88247530e-01\n",
      "    9.93660767e-01   9.59699498e-04   1.84002808e-03   8.17007823e-01\n",
      "    9.08547481e-01   9.86600340e-01   2.41463083e-03   9.79215614e-01\n",
      "    9.89891669e-01   9.90449186e-01   2.72883878e-02   1.37290664e-02\n",
      "    9.80245221e-01   9.37220760e-01   1.32963523e-02   9.93496019e-01\n",
      "    9.46658922e-01   9.11265122e-01   9.49244230e-01   9.96529731e-01\n",
      "    2.02043910e-03   4.58245269e-04   9.32532695e-01   9.92554871e-01\n",
      "    1.83296085e-03   5.91651677e-02   6.82645397e-04   9.67330527e-01\n",
      "    4.76442124e-03   1.51836109e-03   4.52361803e-02   6.71039223e-02\n",
      "    1.26658772e-04   9.73860531e-01   9.88602843e-01   8.53414425e-05\n",
      "    1.60318343e-03   2.28545104e-02   5.03976202e-04   9.78318765e-01\n",
      "    9.93757504e-01   9.99743908e-03   8.92751503e-02   9.92264405e-01\n",
      "    9.77306366e-01   5.44321449e-01   1.37026307e-03   3.39188417e-03\n",
      "    1.66195337e-02   9.98442278e-01   9.22345935e-01   2.00142268e-02\n",
      "    6.10648238e-02   9.68306707e-01   9.57522254e-01   6.09356690e-02\n",
      "    9.96222703e-01   9.40809499e-01   9.99751257e-01   8.11582283e-01\n",
      "    9.57909851e-01   9.78820689e-01   9.95601770e-01   6.94893113e-02\n",
      "    1.03354446e-02   9.87733509e-01   9.96324548e-01   5.00154235e-02\n",
      "    1.85784485e-02   3.64157293e-02   9.32657960e-01   2.54663665e-01\n",
      "    9.93011267e-01   7.86390253e-03   9.63319812e-01   3.88986536e-03\n",
      "    6.50392961e-03   3.14333355e-02   7.19198560e-02   9.47003015e-01\n",
      "    9.66059228e-01   3.08768730e-02   3.71474232e-02   1.63834544e-02\n",
      "    1.23914247e-01   9.66695322e-01   2.58263550e-02   1.77363341e-03\n",
      "    9.79750663e-01   9.66506693e-01   9.90517284e-01   8.94729986e-01\n",
      "    5.20847802e-03   1.86671748e-03   9.37970641e-01   9.80472926e-01\n",
      "    9.55966027e-01   9.85819758e-01   9.95510142e-01   9.63472550e-01\n",
      "    2.01122093e-01   1.04101071e-02   3.72017025e-03   9.25724997e-01\n",
      "    1.20956660e-02   2.16123870e-03   2.69266448e-02   3.09191106e-04\n",
      "    9.93974115e-01   9.94075537e-01   6.43884578e-03   9.72147051e-01\n",
      "    3.16023875e-03   9.42011975e-02   3.31400597e-03   4.34997814e-04\n",
      "    9.98275189e-01   9.91321695e-01   9.81419382e-01   2.78623924e-01\n",
      "    9.94369666e-01   4.16864526e-03   6.69235127e-01   1.09473540e-02\n",
      "    3.70489855e-03   3.87530786e-03   8.42536388e-01   7.40102096e-01\n",
      "    9.94166109e-01   9.99279462e-01   8.05983665e-03   9.92065648e-01\n",
      "    9.93836217e-01   3.44314630e-03   2.87560930e-04   1.26276534e-01\n",
      "    9.79579185e-01   3.15433431e-02   5.25482832e-03   3.37266556e-03\n",
      "    7.15231802e-02   1.63361619e-03   8.03110970e-01   9.65921287e-01\n",
      "    3.53489573e-04   5.81491633e-04   1.21568940e-02   9.79586677e-01\n",
      "    1.28530818e-02   9.74624601e-01   9.92214664e-01   9.70521176e-01\n",
      "    1.54798644e-01   9.88567657e-01   1.81872731e-02   9.68531990e-03\n",
      "    1.84345653e-03   1.70855259e-02   1.65268221e-02   9.63499020e-01\n",
      "    1.89658969e-03   9.95039291e-01   9.60911586e-01   5.86241169e-03\n",
      "    9.65847744e-01   1.96190934e-03   9.65413605e-01   9.14573527e-01\n",
      "    2.55555139e-02   3.25198202e-02   1.76941463e-02   9.88832390e-01\n",
      "    1.70995998e-02   9.91908836e-01   9.27099928e-01   9.81762030e-01\n",
      "    9.75086780e-01   9.58684635e-01   6.73688667e-03   9.77431701e-01\n",
      "    8.17853247e-05   1.87505227e-01   9.96910854e-01   1.72623849e-03\n",
      "    9.95062378e-01   3.73985001e-02   9.92396332e-01   9.32613543e-01\n",
      "    2.44553395e-02   6.49002520e-03   9.30321639e-01   1.10092388e-02\n",
      "    1.75874657e-02   9.96541437e-01   9.68028110e-01   8.91185959e-01\n",
      "    2.15073658e-03   4.29336554e-03   9.50903493e-01   8.11834981e-01\n",
      "    9.74703420e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 2, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 2, 0,\n",
       "       2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0,\n",
       "       0, 2, 2, 0, 0, 2, 2, 2, 0, 0, 0, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 0, 2, 2, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0,\n",
       "       0, 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 0, 0, 0,\n",
       "       0, 2, 2, 0, 2, 0, 0, 0, 0, 2, 2, 2, 0, 2, 0, 2, 0, 0, 0, 2, 2, 2, 2,\n",
       "       0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 2, 2, 2, 0,\n",
       "       2, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 0, 0, 0, 2, 0, 2, 2, 2,\n",
       "       2, 2, 0, 2, 0, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 0, 2, 2, 2, 0, 0, 2,\n",
       "       2, 2], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict(train_x, three_class, p)[1]\n",
    "or_preds = predict(train_x, train_y, parameters)[1]\n",
    "probs = predict(train_x, three_class, p)[2]\n",
    "print (probs)\n",
    "np.argmax(probs, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35796235544 0.489590674873\n"
     ]
    }
   ],
   "source": [
    "or_probs = L_forward(train_x, parameters)[0]\n",
    "three_probs = L_forward(train_x, p)[0]\n",
    "print(three_probs[0][9], three_probs[1][9])\n",
    "def onezero(x):\n",
    "    l = []\n",
    "    if np.max(x)>=0.5:\n",
    "        for i in x:\n",
    "            if i==np.max(x):\n",
    "                l.append(1)\n",
    "            else:\n",
    "                l.append(0)\n",
    "    else:\n",
    "        l = [0]*len(x)\n",
    "    return l\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1\n",
      "  0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
      "  0 1 0 1 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0\n",
      "  1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1\n",
      "  1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1\n",
      "  1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
      "[[0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1\n",
      "  0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
      "  0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0\n",
      "  1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0\n",
      "  1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1\n",
      "  1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
      "  1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 0 0 1\n",
      "  0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(numpy.apply_along_axis(onezero,0,three_probs))\n",
    "print(three_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25312.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1080/128*3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
